################################################################################
# Brings up DCC ETL pipeline:
#
# - Initial hosts allocation
# - Provision Openstack instances and divide into logical groups
# - Create and distribute custom host files for openstack
# - Install Postgresql database server
# - Install ICGC DCC schemas
# - Provision DCC Identifier
# - Provision MongoDB server
# - Provision ElasticSearch server
# - Copy reference data to worker nodes
# - Install LZO on worker nodes
# - Provision Cloudera Manager
# - Install packages on main ETL Node
# - Install etl scripts, reference data on ETL node
# - Install annotator on main ETL node
# - Install reference data for annotator
# - Patch scripts and configuration files for openstack environment
# - Bootstrap dcc-genome on mongodb server
# - Install exporter on main ETL node
# - Install downloader and its workflows into the cluster
#
#
# Note the playbook makes assumptions of which one of hadoop nodes
# is the jobtracker, name-node, quorum..etc
#
#
################################################################################


################################################################################
#
# Post provisioning steps: Do these before starting an ETL run 
#
# - Copy projects' submission files to hdfs
# - Create project.json configuration
# - Download the dictionary for the ETL run
# - Download codelist for the ETL run??
# 
# 
# FIXME: not a good way to handle this
# 1) Patch exporter setenv.sh 
#   - The datatype declaration should only contain the data types that we have, otherwise 
#     the dynamic exporter will throw an exception and halt the overarching script. This
#     is difficult to do programmatically as we don't know that projects have what types.
#
# 2) (Re)create oozie workflow for coordinator (archivecleaner)
#   - Goto a oozie server
#   - Copy job.properties from HDFS     hadoop fs -copyToLocal /user/downloader/workflows/archivecleaner-main/job.properties job.properties
#   - Get job id if exists     oozie jobs -oozie http://localhost:11000/oozie -jobtype coordinator
#   - Kill job if exists       oozie job -oozie http://localhost:11000/oozie -kill <Coordinator job_id from above>
#   - Install coordinator      oozie job -oozie http://localhost:11000/oozie -config job.properties -submit
#   - Start coordinator        oozie job -oozie http://localhost:11000/oozie -start <New coordinator job_id>
#
################################################################################

- name: Create the host file to be uploaded to instances
  hosts: localhost
  gather_facts: False
  tasks:
    - name: "Remove previous host file if exist"
      shell: rm -f bootstrap_hosts
      delegate_to: localhost
    - name: "Create new host file with localhost entry"
      shell: echo "127.0.0.1 localhost" > bootstrap_hosts
      delegate_to: localhost

- name: Create OpenStack Instances
  hosts: etl-all
  user: root
  gather_facts: False
  serial: 1
  vars_files:
    - "vars/main.yml"
  tasks:
    - include: tasks/create-instances.yml
    - name: Add public ip to hostgroup
      # this hack allows adding each hostname to a group that will be used downstream. the regex strips "dcc-" from beginning
      # and potential "-xx" from the end of hostname and uses that as a group name, allowing us to use group names 
      # like etl-postgres or etl-worker in include statements. ex. dcc-etl-worker-01 -> etl-worker
      add_host: name={{ ip_data.public_ip }}
                groups={{ inventory_hostname | regex_replace('^dcc-(etl-\w*)(-\d*$)?', '\\1') }},all_instances
                hostname={{ inventory_hostname }}
    - name: Writing private IPs to host file
      shell: echo "{{ creation_data.private_ip }} {{ inventory_hostname }}" >> bootstrap_hosts
      delegate_to: localhost

- name: Check if SSH is ready on servers
  hosts: all_instances
  gather_facts: False
  tasks:
    - name: Wait for ssh
      wait_for: host={{ inventory_hostname }} 
                port=22 
                timeout=900
      delegate_to: localhost

- name: Replace /etc/hosts with bootstrapped hosts file
  hosts: all_instances
  sudo: True
  serial: 1
  vars_files:
    - "vars/main.yml"
  vars:
    host_key_checking: False
    ansible_ssh_user: ubuntu
  tasks:
    - name: Copying... 
      copy: src=bootstrap_hosts 
            dest=/etc/hosts 
            owner=root 
            group=root


- include: etl-elasticsearch.yml
- include: etl-postgres.yml
- include: etl-identifier.yml
- include: etl-mongo.yml
- include: etl-cm.yml
- include: etl-main.yml
- include: etl-worker.yml
