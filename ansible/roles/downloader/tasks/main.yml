- name: Clean copy
  file: path="{{ downloader_root }}"
        state=absent

- name: Create directories
  file: path="{{ item }}" 
        state="directory" 
        owner="{{ dcc_user }}"
  with_items:
    - "{{ dcc_user_directory }}"
    - "{{ downloader_root }}"

- name: get dcc-downloader tarball from artifactory
  get_url:  url="{{ downloader_dist_url }}" 
            dest="{{ staging_dir }}/{{ downloader_dist_filename }}" 
            mode=644

- name: Extract dcc-downloader ==> /u/dcc_dev/dcc_downloader
  unarchive:  src="{{ staging_dir }}/{{ downloader_dist_filename }}"
              dest="{{ downloader_root }}"
              copy=no

# Patch oozie workflow configurations
- name: Patch archive
  template: src="archive/config-default.xml.j2" 
            dest="{{ downloader_root}}/workflows/archive/config-default.xml" 
            mode=644 
            force=yes

- name: Patch archive-main
  template: src="archive-main/config-default.xml.j2" 
            dest="{{ downloader_root}}/workflows/archive-main/config-default.xml" 
            mode=644 
            force=yes

- name: Patch archivecleaner
  template: src="archivecleaner/config-default.xml.j2" 
            dest="{{ downloader_root}}/workflows/archivecleaner/config-default.xml" 
            mode=644 
            force=yes

- name: Patch archivecleaner-main
  template: src="archivecleaner-main/config-default.xml.j2" 
            dest="{{ downloader_root}}/workflows/archivecleaner-main/config-default.xml" 
            mode=644 
            force=yes

- name: Patch archivecleaner-main job property (for installing)
  template: src="archivecleaner-main/job.properties.j2" 
            dest="{{ downloader_root}}/workflows/archivecleaner-main/job.properties" 
            mode=644 
            force=yes


# Copy to HDFS
# FIXME: Need to take care of coordinator jobs
# - name: Remove workflow dir
#   shell: "HADOOP_USER_NAME=hdfs hadoop fs -rm -r -skipTrash /user/downloader/workflows"
#   ignore_errors: yes

- name: Create workflow dir
  shell: "HADOOP_USER_NAME=hdfs hadoop fs -mkdir /user/downloader/workflows"
  ignore_errors: yes

- name: Copy archive
  shell: "HADOOP_USER_NAME=hdfs hadoop fs -copyFromLocal -p {{ downloader_root }}/workflows/archive /user/downloader/workflows/archive"
  ignore_errors: yes

- name: Copy archive-main
  shell: "HADOOP_USER_NAME=hdfs hadoop fs -copyFromLocal -p {{ downloader_root }}/workflows/archive-main /user/downloader/workflows/archive-main"
  ignore_errors: yes

- name: Copy archivecleaner
  shell: "HADOOP_USER_NAME=hdfs hadoop fs -copyFromLocal -p {{ downloader_root }}/workflows/archivecleaner /user/downloader/workflows/archivecleaner"
  ignore_errors: yes

- name: Copy archivecleaner-main
  shell: "HADOOP_USER_NAME=hdfs hadoop fs -copyFromLocal -p {{ downloader_root }}/workflows/archivecleaner-main /user/downloader/workflows/archivecleaner-main"
  ignore_errors: yes

- name: Change ownership
  shell: "HADOOP_USER_NAME=hdfs hadoop fs -chown -R downloader /user/downloader/workflows"
  ignore_errors: yes

- name: Create dynamic lib directory
  shell: "HADOOP_USER_NAME=hdfs hadoop fs -mkdir -p /hbase_lib"
  ignore_errors: yes

- name: Copy over filter.jar
  shell: "HADOOP_USER_NAME=hdfs hadoop fs -copyFromLocal {{ downloader_root }}/hbase-lib/filter.jar /hbase_lib/filter.jar"
  ignore_errors: yes


# Copy downloader libs to hdfs for oozie workflows
# FIXME: Maybe we can just specify a lib path instead??
- name: Create lib dir
  shell: "hadoop fs -mkdir /user/downloader/workflows/{{ item }}/lib"
  with_items:
    #- "archive"
    - "archivecleaner"
  environment: env
  ignore_errors: yes

- name: Copy lib to hdfs
  shell: "hadoop fs -copyFromLocal {{ downloader_root }}/workflows/archive/lib/dcc-downloader.jar /user/downloader/workflows/{{ item }}/lib/dcc-downloader.jar"
  with_items:
    #- "archive"
    - "archivecleaner"
  environment: env
  ignore_errors: yes
  
- name: Make sure dcc_dev owns everything
  file: path="{{ downloader_root }}" 
        owner={{ dcc_user }}
        recusre=yes
